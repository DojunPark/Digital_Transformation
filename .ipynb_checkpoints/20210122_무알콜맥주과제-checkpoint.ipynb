{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제: 무알콜 맥주 소비시장 분석 및 마케팅 전략\n",
    "- 클라우드 제로의 문제점\n",
    "- 판매 전략\n",
    "- ppt 5~10이내\n",
    "- 10~15분 발표 분량\n",
    "- 뉴스 / 블로그 / 쇼핑몰 리뷰 크롤링 진행\n",
    "- 워드 클라우드, 차트로 시각화\n",
    "- 제품 분석, 문제점, 시장 현황, 판매전략\n",
    "- 발표일: 수요일 오후\n",
    "- 발표에 대해 개별적 투표 진행\n",
    "- 디자인은 보지 않고, 기술 적용에 대해 평가\n",
    "- 개별 인원이 어떤 작업을 진행하였는지 명시할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 업무분장\n",
    "1. 크롤링\n",
    "2. 시각화 (워드클라우드, 차트)\n",
    "3. 분석 및 전략\n",
    "4. 발표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 블로그 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/dojun\\Documents/api_keys/naver_api.txt') as f:\n",
    "    client_id = f.readline().replace('\\n', '')\n",
    "    client_secret = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from urllib.request import quote\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "keyword = '클라우드 제로'\n",
    "url = 'https://openapi.naver.com/v1/search/blog.json?query=' + keyword + '&display=100&start=1'\n",
    "html = requests.get(url,headers = {\"X-Naver-Client-Id\" : client_id,\n",
    "                                   \"X-Naver-Client-Secret\" : client_secret})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "json1 = html.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lastBuildDate', 'total', 'start', 'display', 'items'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json1['items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for item in json1['items']:\n",
    "    link = item['link']\n",
    "    links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "crawling...: 100%|███████████████████████████████████████████████████████████████████| 100/100 [04:25<00:00,  2.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "regex = re.compile('.*postViewArea.*')\n",
    "\n",
    "driver = webdriver.Chrome('./py_data/crawling/chromedriver.exe')\n",
    "\n",
    "txt = ''\n",
    "for link in tqdm(links, 'crawling...'):\n",
    "    if 'tistory' not in link:\n",
    "        driver.get(link)\n",
    "        driver.implicitly_wait(5)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        driver.switch_to.frame('mainFrame')\n",
    "        html = driver.page_source\n",
    "        soup = bs(html, 'html.parser')\n",
    "        \n",
    "        text = soup.find('div', class_=regex).text\n",
    "        text = text.replace('\\n\\n', '') + '\\n'\n",
    "        \n",
    "        txt += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blog.naver.com/jdb1313?Redirect=Log&logNo=222203291619\n"
     ]
    }
   ],
   "source": [
    "print(links[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = re.sub('[^ㄱ-힣 ]', '', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./py_data/crawling/클라우드제로/네이버블로그_클라우드제로.txt', 'w') as f:\n",
    "    f.write(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버쇼핑 클라우드제로 상품평"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "crawling...: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:15<00:00,  1.55s/it]\n",
      "crawling...: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.14s/it]\n",
      "crawling...: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.13s/it]\n",
      "crawling...: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.14s/it]\n",
      "crawling...: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.14s/it]\n",
      "crawling...: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.13s/it]\n",
      "crawling...:  50%|███████████████████████████████████                                   | 5/10 [00:05<00:05,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "regex = re.compile('.*reviewItems_review.*')\n",
    "url = 'https://search.shopping.naver.com/catalog/17652239575?query=%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C%20%EC%A0%9C%EB%A1%9C&NaPm=ct%3Dkk85c8hk%7Cci%3D3ae9c8b7caf0a6dbea0fdcc60c883afb8432afee%7Ctr%3Dslsl%7Csn%3D95694%7Chk%3D769ae2343aff7280004d8525d59e6a0ef01df71c#'\n",
    "button = '#section_review > div.pagination_pagination__2M9a4 > a:nth-child('\n",
    "driver = webdriver.Chrome('./py_data/crawling/chromedriver.exe')\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "txt = ''\n",
    "page = 1 \n",
    "# 첫번째 페이지(1~10)\n",
    "for i in tqdm(range(1, 11), 'crawling...'):\n",
    "    driver.implicitly_wait(5)\n",
    "    time.sleep(1)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    div = soup.find_all('div', class_=regex)\n",
    "    for d in div:\n",
    "        tmp = d.text\n",
    "        txt += tmp\n",
    "        \n",
    "    if i < 11:\n",
    "        css_button = button + str(i) + ')'\n",
    "    elif i == 11:\n",
    "        css_button = '#section_review > div.pagination_pagination__2M9a4 > a.pagination_next__3ycRH'\n",
    "    \n",
    "    driver.find_element_by_css_selector(css_button).click()\n",
    "    page += 1\n",
    "    \n",
    "# 두번째 페이지 이후(11~66)\n",
    "while page <= 65:\n",
    "    for i in tqdm(range(4, 14), 'crawling...'):\n",
    "        if page == 66:\n",
    "            break\n",
    "            \n",
    "        driver.implicitly_wait(5)\n",
    "        time.sleep(1)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = bs(html, 'html.parser')\n",
    "\n",
    "        div = soup.find_all('div', class_=regex)\n",
    "        for d in div:\n",
    "            tmp = d.text\n",
    "            txt += tmp\n",
    "\n",
    "        if i <= 12:\n",
    "            css_button = button + str(i) + ')'\n",
    "        elif i == 13:\n",
    "            css_button = '#section_review > div.pagination_pagination__2M9a4 > a.pagination_next__3ycRH'\n",
    "\n",
    "        driver.find_element_by_css_selector(css_button).click()\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = re.sub('[^ㄱ-힣 ]', '', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./py_data/crawling/클라우드제로/네이버쇼핑_클라우드제로.txt', 'w') as f:\n",
    "    f.write(tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
